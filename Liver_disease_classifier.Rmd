---
title: "Liver disease prediction"
author: "Moschos Evangelos-Jason"
abstract: "In this document, we are presenting our analysis on a liver disease classifier, based on the indian liver disease dataset found on Kaggle. The project is conducted for the Harvard Data science professional program."
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

```{r results='hide', echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}

# Install all needed libraries if not there

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(MLmetrics)) install.packages("MLmetrics", repos = "http://cran.us.r-project.org")
if(!require(ROCR)) install.packages("ROCR", repos = "http://cran.us.r-project.org")
if(!require(polycor)) install.packages("polycor", repos = "http://cran.us.r-project.org")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(varhandle)) install.packages("varhandle", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(doSNOW)) install.packages("doSNOW", repos = "http://cran.us.r-project.org")
if(!require(fastAdaboost)) install.packages("fastAdaboost", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")

```

```{r results='hide', echo=FALSE, message=FALSE,include=TRUE, warning=FALSE}

## Library loading

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(randomForest)
library(rpart)
library(xgboost)
library(ggplot2)
library(stringr)
library(ggpubr)
library(tinytex)
library(kableExtra)
library(httr)
library(RCurl)
library(ROCR)
library(polycor)
library(nnet)
library(varhandle)
library(reshape2)
library(e1071)
library(gbm)
library(tinytex)
library(doSNOW)


```


\newpage

# Executive summary {-}

The prevalence of liver diseases has increased tremendously the past few years, due to alcohol abuse, illegal substances, poisoned or contamined food etc. It is often characterized as a burden on both health sciences and economies and has a moderate mortality rate, amongst chronic diseases. 

As part of the Harvard data science program, we will be attempting to create a classifier for detecting liver disease, based on a dataset for Indian patients. The dataset we are using is found at https://www.kaggle.com/uciml/indian-liver-patient-records. The aim of the project is to be able to predict (based on some characteristics) the existance of a liver disease in people. These characteristics include some general demographic characteristics and some blood-test results. Our dataset is slightly imbalanced, consisting of 72% liver patients and 28% healthy individuals.

In our exploratory analysis, we firstly examined the 10 variables (Age, Gender, Total Bilirubin, Direct Billirubin, Alkaline Phospotase, Alamine Aminotransferase, Aspartate Aminotransfease, Total Proteins, Albumin and Albumin to Globulin ratio) seperately. Our results indicate that age and gender do not appear to have substantially different distributions on the prevalence of liver disease, although women were slightly less prone to it. This statement is also in-line with the literature online, which states that liver disease occur throughout the world, irregardless of sex, region and age. 

For the other 8 variables, due to lack of domain knowledge, we examined the distribution of each variable for both groups (healthy and sick) seperately, as well as the correlation between the variables. We identified that while extreme values in test results are correlated with the existence of the disease, it is hard to distinguish between a healthy and a sick individual for values within the normal range. As several variables were highly correlated with each other, we decided to remove the Direct Bilirubin, the Alamine aminotranferase and the Albumin and use all the remaining variables as features.

Additionally, we created a new variable, indicating the total number of extreme values (i.e. blood test results outside the normal range, defined solely on the train set). Finally, we also decided to preprocess our dataset by scaling and centering all the variables.

Based on our EDA, we have developed nine models that consider these variables. The best performing model we developed was a Random forest algorithm that achieved an: 

* **Accuracy of 81.03% in the test-set** 

To evaluate the feature engineering, we have re-trained a random forest model (10th model), on the unprocessed dataset; the accuracy of the second Random forest algorithm was **75.86%**, and thus the features selected were beneficial to the overall modelling process. 

The analysis shows significant improvement over simpler methods (13% improvement), but several other factors can be incorporated to further improve the accuracy of the model.



\newpage

# Introduction

As part of the Harvard data science professional program, we will attempt to create a classifier for detecting liver disease in individuals. In the next chapters, our analysis is structured as follows:

* Chapter 2: Dataset loading/generation 

* Chapter 3: Exploratory Data Analysis 

* Chapter 4: Feature selection

* Chapter 5: Analysis: Model building & Testing

* Chapter 6: Conclusion 


In chapter 2, the generation of the dataset is explained, and some basic dimensions of the data are explored.

In chapter 3, an Exploratory data analysis is conducted, to identify interesting features in the dataset, and help us select the appropriate variables in our models.

In chapter 4, the features of the models are selected based on the EDA. Some pre-processing is also performed.

In chapter 5, the basic models are built, based on the characteristics identified in chapter 4. All the models are also evaluated.

In chapter 6, an overview of the results is presented, limitations of the project are discussed, and direction for future research is provided.


\newpage
# Dataset generation/loading

In this chapter, we will describe the dataset generation process, some useful (general) statistics for the dataset and some first insights we have.

The entire dataset is found at https://www.kaggle.com/uciml/indian-liver-patient-records, but it is automatically downloaded in our code through https://github.com/jmoschos/Liver_disease/blob/master/indian_liver_patient.csv.

```{r, include = FALSE, eval = TRUE}

# Dataset loading

##Automatic Data reading from my git repository jmoschos

raw_data<-read.csv("https://raw.githubusercontent.com/jmoschos/Liver_disease/master/indian_liver_patient.csv")

```

## Dataset loading and basic manipulations

We begin by examining the different variables:


```{r, echo=FALSE, eval = TRUE}

raw_data%>%
  head(5)%>%
  kable(align=rep('c', ncol(raw_data))) %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = FALSE,
                 latex_options = "scale_down")
```


There are 11 variables:

* Age: 
* Gender: 
* Total Bilirubin
* Direct Bilirubin
* Alkaline Phosphotase
* Alamine Aminotransferase
* Aspartate Aminotransferase
* Total Proteins
* Albumin
* Albumin-to-Globulin ratio
* Dataset: Target Variable

Based on the data, we see that the majority of the variables are continuous with the exception of the target variable and the gender variable. A summary of the variable types is also illustrated in the following figure:


```{r, echo=FALSE, eval = TRUE}

## Checking the class of the variables:

str(raw_data)

```


Additionally, based on the documentation, we see that the target variable is 1 if the person is sick and 2 if he is healthy. We are going to change the convention to 1 if the person is sick and 0 if he is healthy. For the gender variable, we are also going to change it to a binary variable with 0 representing male and 1 female. Finally, we are going to convert these 2 variables into factors, as a preparation for our machine learning models.

```{r, echo=FALSE, eval = TRUE}
# Basic manipulations
## Convert name of target variable to y and change its value to binary

raw_data<-raw_data%>%
  rename(y=Dataset)%>%
  mutate(y=ifelse(y==2,0,y))  ## Changing target variable to 0,1 {0 = No disease, 1 = Liver disease}  
                              ## In original file its encoded {1=disease, 2 = No disease}

## Checking that the conversion is done corectly: We need 416 patients with liver disease (y=1) and 167 without (y=0)
paste0("Number of healthy individuals:  ",sum(raw_data$y==0))
paste0("Number of sick individuals:  ",sum(raw_data$y==1))

## Making the y and gender variables into factors (for gender, we also change it to a binary variable)
raw_data<-raw_data%>%
  mutate(y=as.factor(y))%>%
  mutate(Gender=ifelse(Gender=="Male","0","1"))%>%
  mutate(Gender=as.factor(Gender))

```

We are also going to check for missing values in our dataset: 

```{r, echo=FALSE, eval = TRUE}
## Missing values ? 

sapply(raw_data, function(x) sum(is.na(x)))%>%kable()       ## only albumin and globulin ratio has some NA's 
```


Based on the previous table, only one variable has (4) missing values. Due to the low number of missing values, we could consider removing these records, but in our implementation, we have chosen to replace these values with the average of all other values for that variable.

```{r, echo=FALSE, eval = TRUE}
## We will replace the NAs with the mean of the rest of the observations.If there were more outliers, a better choice might have been the median.

raw_data$Albumin_and_Globulin_Ratio<-ifelse(is.na(raw_data$Albumin_and_Globulin_Ratio), mean(raw_data$Albumin_and_Globulin_Ratio,na.rm=TRUE),raw_data$Albumin_and_Globulin_Ratio)     ## if the value is NA, replace it with the mean of the rest of the values                                                       ## (NAs excluded with na.rm argument, otherwise keep it as is)
```


## Dataset generation 

For model building purposes, we are going to split the initial dataset into a train set and a test set. We have decided to use a 80/20 split rule for these sets. For the sake of reproducability, we have added a seed to both the set generation and all subsequent algorithms, where randomness plays a role.

```{r, echo=FALSE, eval = TRUE}

## Data splitting: 80% for train and 20% for test

##Seed for reproducability
set.seed(1)

## Making an index for the train set.
train_index<-createDataPartition(raw_data$y, p = 0.8, times = 1, list= FALSE)

## Creating the subsets for train and test
train<-raw_data[train_index,]
test<-raw_data[-train_index,]

## Removing the index. No longer required.
rm(train_index)
```


\newpage

# Exploratory Data analysis

In this section, we are presenting our exploratory data analysis (EDA). From the variables in our dataset, described in the previous section, we will examine all of them seperately and then calculate the correlation between them.

## Target Variable: y

We will start by examining the distribution of healthy vs. sick individuals in our dataset.


```{r, echo=FALSE, eval = TRUE,out = '130%'}
raw_data%>%
  ggplot(aes(y))+
  geom_bar()+
  labs(title = "Liver disease prevalence",
       x = "Disease",
       y= "Number of people")+
  scale_x_discrete(labels = c("No disease", "Liver disease"))
```

It appears that our dataset is imbalanced with only 28% healthy individuals and 72% sick. 

\newpage

## Gender analysis

Literature suggests that the disease appears in both genders . To examine this statement, we are going to plot the number of instances of healthy vs. unhealthy individuals for two genders.

```{r, echo=FALSE, eval = TRUE,out.width = '80%'}
## Does gender affect the disease?

raw_data%>%
  group_by(Gender,y)%>%
  ggplot(aes(x=Gender,fill=y))+
  geom_bar()+
  labs( title = "Prevalence of disease based on gender",
        x = "Gender",
        y = "Number of instances")+
  scale_fill_discrete(name = "Category", labels = c("No disease", "Liver disease"))+
  scale_x_discrete(labels = c("Male","Female"))

## Its hard to compare between the two categories, but females represent a lower percentage in our dataset.
```

Based on the the plot, we can see that the dataset contains mostly male data, but its hard to compare the prevalence of the disease between the  two genders. Therefore, we are going to scale our data to be able to do the comparison.

```{r, echo=FALSE, eval = TRUE,out.width = '80%'}
## Lets look at percentage data instead, for comparison between the two groups.


raw_data%>%
  group_by(Gender,y)%>%
  ggplot(aes(x=Gender,fill=y))+
  geom_bar(position="fill")+
  labs( title = "Scaled Prevalence of disease based on gender",
        x = "Gender",
        y = "Percentage of instances")+
  scale_fill_discrete(name = "Category", labels = c("No disease", "Liver disease"))+
  scale_x_discrete(labels = c("Male","Female"))


## It appears that females (while being under-represented, have a slightly lower disease to no-disease ratio)
```

The graph indicates that both genders behave similarly, although females have a slightly lower rate of disease. 

## Age analysis

Continuing our EDA, we are going to explore the age distribution across our dataset:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE,out.width = '80%'}
## Lets now examine the age distribution in our dataset.

raw_data%>%
  ggplot(aes(Age))+
  geom_histogram(col="blue", fill = "yellow")+
  labs( title = "Age Distribution",
        x = "Age",
        y = "Number of people")

## Age is a rather wide distribution (0-90 year old) with more observations centered around the mean
```

The graph shows a wide distribution, across nearly all ages that is centered around the middle. We are now going to plot the histogram of ages but also indicate the target variable y.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE,out.width = '90%'}
## Lets check how the age distribution differs for patients and non-patients

raw_data%>%
  ggplot(aes(x=Age, fill = y))+
  geom_histogram(col="black")+
  labs( title = "Age Distribution vs. Disease",
        x = "Age",
        y = "Number of people")+
  scale_fill_discrete(name = "Category", labels = c("No disease", "Liver disease"))

## We observe a similar distribution based on age.  
```

It appears that both healthy and sick individuals follow similar age distribution. 

## Blood test results

Due to lack of domain knowledge, exploring the remaining 8 variables is not a straightforward process. We are going to start by plotting scatter plots of the individual variables, but also add jitter in our plots, in order for the individual dots to be seperated.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Scatter plots with jitter:

##Function that based on an index i produces the required plot
plotVar<-function(i){
  
  ggplot(aes(x=raw_data[,i],y=y),data=raw_data)+
    geom_point(position="jitter",  col = "blue")+
    labs( x = names(raw_data[i]))
}

## i from 3 (excl. gender and age) to max column - 1 (exluding the target variable)
i<-3:(ncol(raw_data)-1)

## Creating the plots and saving them to plots
plots<-map(i,plotVar)

## arranging them in a grid
ggarrange(plotlist=plots,ncol=2, nrow=4)

## removing plots
rm(plots)
```


From these 8 plots, we can see the following:

* **Total Bilirubin and Direct Bilirubin have similar behaviour**: For healthy invididuals, both metrics have small values, and for sick individuals we have a wide range of values (including the range of healthy people).

* **Alkaline phosphotase has a similar behaviour to the Bilirubins but to a smaller extent**: While for the bilirubins we can see up to 20 times higher values, in Phospotase we see up to 4 times higher values in liver patients. 

* **Total Proteins, Albumin and Albumin-to-Globulin ratio have similar graphs**, where both healthy and sick people are centered around a value but the range for sick people is (slightly) larger.

* **The two aminotransferases also have the same behaviour as the bilirubins and the alkaline** but with fewer outliers.


To continue the exploration we are going to plot the histograms for the same variables:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
##Histograms: We exclude the Gender variable that is categorical (factor)

plotHist<-function(i){
  
  ggplot(aes(x=raw_data[,i],fill=y),data=raw_data)+
    geom_histogram(col="yellow")+
    labs( x = names(raw_data[i]),
          y = "Count")+
    scale_fill_discrete(name = "Category", labels = c("No disease", "Liver disease"))
}


## index from 1 to max column -1 EXCLUDING gender variable (i==2)
i<-c(3:(ncol(raw_data)-1))

## Creating the plots
plots<-map(i,plotHist)

##Arranging the plots
ggarrange(plotlist=plots,ncol=2, nrow=4)

##removing the plots and index variable
rm(plots,i)
```

From the histograms, we observe that:

* Albumin, Albumin-to-globulin ratio and Total proteins follow similar distributions.
* The other 5 variables have very similar distributions. Alkaline Phosphotase is slightly different than the rest, as it has very few observations at 0.
* In all the variables, it seems like having a liver disease has a bigger range of values, compared to not having the disease and the range of values for sick individuals includes the range of values for healthy individuals.
* There are patients with "normal" indicators that have the disease.


\newpage
## Correlation between variables

Based on the histograms and scatterplots, it appears that some of the variables have similar characteristics. To verify that, we are going to calculate the correlation matrix for all continuous variables (and exclude the gender).

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

## Lets check the correlation between all the continuous variables

## Picking only the continuous variables
corvar<-raw_data[,c(1,3:10)]

##Creating the correlation matrix
cormatrix<-cor(corvar)

## we are going to use only the upper part of the matrix and leave the rest blank
cormatrix[upper.tri(cormatrix)]<-NA

## Using reshape library, we are "melting" the matrix into a useable form for a heatmap
cormatrix<-melt(cormatrix,na.rm = TRUE)

## Plotting the heatmap of correlation
cormatrix%>%
  ggplot(aes(x=Var1,y=Var2,fill=value))+
  geom_tile(color="white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1))+
  coord_fixed()

rm(cormatrix,corvar)
```

Similar to our observations:

* Albumin is highly correlated with both Albumin-to-globulin ration and Total proteins.
* Direct Bilirubin is highly correlated with Total Bilirubin.
* Alamine aminotransferace is highly correlated with Aspartate aminotransferace.

\newpage
# Feature selection 

Based on our EDA, we are now going to select the features that will be used in our models.

To Test the effectiveness of the steps performed in this section, we are going to save the original train and test datasets and re-train the best performing model on those. Our comparison will be between:

* The best performing model with the feature-engineered dataset [case 1]
* The same model as case 1, but with the original (no changes made) dataset [case 2]

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

## Before doing any manipulation to the original dataset, we are going to save them. We will utilize them for benchmarking later in the process.
trainS<-train
testS<-test

```

## Variable removal

In the correlation matrix, we observed that some variables are highly correlated with each other. We are going to remove some of the variables that convey the same type of information.

Firstly, we are going to look at Bilirubin (Total vs. Direct). 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Some variables are highly correlated with each other. We are going to select the variables with the minimum variability and remove them from our sets for the model training.

## Examining Total vs. Direct Bilirubin

p1<-train%>%
  ggplot(aes(x="",y=train[,3]))+
  geom_boxplot()+
  geom_point(position="jitter")+
  labs(title="Total Bilirubin")+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())+
  ylim(0,80)

p2<-train%>%
  ggplot(aes(x="",y=train[,4]))+
  geom_boxplot()+
  geom_point(position="jitter")+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())+
  ylim(0,80)+
  labs(title="Direct Bilirubin")

## Arranging in 1 plot
ggarrange(p1,p2,ncol=2,nrow=1)

## Removing subplots
rm(p1,p2)

## Total bilirubin has a higher variation, so we will keep this
```

Based on this graph, Total Bilirubin has a higher variation, and thus we are going to remove Direct Bilirubin from our dataset.On a sidenote, Total Bilirubin is the sum of Direct and Indirect bilirubin, which is why there is such a high correlation between the two.

We are now going to check Alamine aminotransferase and Aspartate aminotransferase:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Examining Alamine aminotransferase vs. Aspartate aminostransferase

p1<-train%>%
  ggplot(aes(x="",y=train[,6]))+
  geom_boxplot()+
  geom_point(position="jitter")+
  labs(title="Alamine aminotransferase ")+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())+
ylim(0,5000)

p2<-train%>%
  ggplot(aes(x="",y=train[,7]))+
  geom_boxplot()+
  geom_point(position="jitter")+
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())+
  ylim(0,5000)+
  labs(title=" Aspartate aminostransferase")

## arranging in 1 plot for comparison
ggarrange(p1,p2,ncol=2,nrow=1)

## Removing plots
rm(p1,p2)

## Aspartate has a higher variation, so we will remove alamine
```

Similarly to the bilirubins, we observe that Aspartate has a much higher variation, and therefore we are going to remove Alamine.

Finally, for albumin, since its highly correlated with both Albumin to Globulin ratio and Total proteins, we are going to remove it, to solve both problems at once.

## New variable creation: Number of extremes

Besides the existing variables in our dataset, we believe it might be of interest to add a new variable: Number of extreme blood test result values. The motivation behind this choice is that most healthy patients will not have any extreme values in their blood test results, while sick individuals will have at least some indication of a liver disease.

To create this variable we will perform the following steps: 

* First calculate the normal range of values on the **train set only** for the **healthy individuals**. In a real scenario we would not have the test set output (sick or healthy), and therefore we cannot base our analysis on that set. 
* Create a dummy variable for each blood test result (8 variables)
* Fill the dummy variable with 1 if the blood test result of that individual is outside the limits calculated (below lower limit, or above upper limit).
* Sum the rows

The final vector contains for each individual the total number of blood test result values that is outside the "normal range" (which again is calculated **ONLY on the train set**). Our hyptothesis is that people with a large number of extreme values are sick, while people with lower number (or even 0) are more likely to be healthy.


```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Adding a new variable: Number of extremes for blood test results

## it seems that most sick patients have values that are much more extreme than that of healthy patients. It might be interesting to explore for the sick patients, how many values of the continuous variables are outside the normal range (normal range is defined as the range observed in healthy patients.)

##dummy variable = number of "normal values for other variables". If a patient has a value within the range of a normal patient y=0. In the end we sum the values of these 8 dummy variables for the total number of out-of-bounds variables.

## Find the min and max for healthy patients     ## the range is calculated ONLY ON THE TRAIN DATA. We do not use the unseen data for our calculations

min<-apply(train[train$y==0,],2,min)
max<-apply(train[train$y==0,],2,max)

## Make these limits into 1 dataframe
lims<-data.frame(min=min,max=max)

##removing min and max
rm(min,max)

##Removing age,gender and target variable   --> We only care for the blood results
lims<-lims[3:10,]     

##Making the lims into numeric (and not factor)
lims<-unfactor(lims) 

##Function to create the 8 dummy variables per patients. Checking if the reading is less than min of a healthy patients or more than the max of a healthy patient. If it is outside the bounds 1, otherwise 0.

y<-function(i,data){ifelse(data[,i+2]<lims$min[i] | data[,i+2]>lims$max[i],1,0)}

## Index i for 1 to number of rows (i.e. variables) in lims ==8
i<-1:nrow(lims)
d_train<-sapply(i,y,data=train)       ##calculating the new variables for the train set
d_test<-sapply(i,y,data=test)         ## Calculating the new variables for the test set


##Calculating the total number of "normal values" for any given patient
d_train<-rowSums(d_train)
d_test<-rowSums(d_test)


## Binding the original datasets with the new variable we created
train<-cbind(train,d_train)
test<-cbind(test,d_test)
train<-train%>%
  rename(Number_of_extremes=d_train)

test<-test%>%
  rename(Number_of_extremes=d_test)


## Removing the un-needed variables
rm(d_test,d_train,i,lims)
```

The two histograms showing the distribution of Extreme values for healthy and sick individuals are depicted in the following figures:

For the train set:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE,fig.height=2}

p0<-train%>%
  filter(y==0)%>%
  ggplot(aes(Number_of_extremes))+
  geom_histogram()+
  xlim(-1,6)

p1<-train%>%
  filter(y==1)%>%
  ggplot(aes(Number_of_extremes))+
  geom_histogram()+
  xlim(-1,6)

ggarrange(p0,p1,ncol=2,nrow=1)

```

For the test set:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE,fig.height=2}

p0<-test%>%
  filter(y==0)%>%
  ggplot(aes(Number_of_extremes))+
  geom_histogram()+
  xlim(-1,6)

p1<-test%>%
  filter(y==1)%>%
  ggplot(aes(Number_of_extremes))+
  geom_histogram()+
  xlim(-1,6)

ggarrange(p0,p1,ncol=2,nrow=1)

```

As can be seen for the train set, we have, as expected, only 0 values for the healthy individuals, since the bounds we used were created from this set. On the other hand, for individuals with liver disease, we see a different distribution (but also a lot of 0 values i.e. all blood test values within what we defined as normal range).

On the test set, we observe similar results, as the train set, but we should underline the fact that there was a chance to observe a different distribution (not all 0s) for healthy individuals, as the values for the bounds were created based on the train set and were agnostic of any "extreme" values in the test set.


## Features of our models

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Our final feature selection includes: Age, Gender, Total Bilirubin, Alkaline Phosphotase, Aspartate Aminostransferase, Total Proteins and Albumin/Globulin Ratio, Number of extremes

train<-train[,c(1,2,3,5,7,8,10,11,12)]
test<-test[,c(1,2,3,5,7,8,10,11,12)]
```

Based on our feature engineering, the final features for our models are:

* Age
* Gender
* Total Bilirubin
* Alkaline Phosphotase
* Aspartate Aminotransferase
* Total Proteins
* Albumin to Blobulin ratio
* Number of extremes

\newpage
# Analysis: Model building and testing

In this section, we are presenting the different models developed and their performance. We are also discussing the metric used for evaluation and the validation method for tuning.

## Data pre-processing

Before implementing any ML algorithms, we are going to scale and center the data with the help of the **Preprocess** function on caret.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}
## Preprocessing the data: Center and scaling

preProcValues <- preProcess(train, method = c("center", "scale"))

## Calculate thew new centered/scaled values
trainTransformed <- predict(preProcValues, train)
testTransformed <- predict(preProcValues, test)

## Set the train and test set equal to the processed data.
train<-trainTransformed
test<-testTransformed
```

In the following table, a sample of our scaled data is presented:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

train%>%
  head(5)%>%
  kable(align=rep('c', ncol(train))) %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 8,
                 full_width = FALSE,
                 latex_options = "scale_down")
```

## Metric for performance

Typically, in medical applications false negatives are far more important than false positives and therefore, metrics such as F1 score are utilized. In our implementation, we are going to train our models with the **F1 score** as the metric, but also track the overall **accuracy**. 

Due to the fact that our dataset is imbalanced with the sick individuals being more than 70%, it is likely that some models might classify all people as sick and therefore the F1 score cannot be calculated.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

## Metric = Accuracy
metric="F1"

```

## Validation method and tuning

For tuning our algorithms, we have selected **10-fold cross-validation** for all methods. The final reported performance (accuracy, F1) is based on the test set, after selecting the optimal parameters (from cross validation on the train set).

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

## Control = 10-fold cross validation
control<-trainControl(method="cv",number=10)

```

## Model 1: Naive model

For the first model, which we will use as benchmark, we are going to implement a naive-approach: classify every patient as having the disease. Since our dataset is imbalanced (favoring disease), our accuracy will be high (the f1 score is not defined in this case).

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

## Naive model - Always liver disease

y_hat_naive<-rep.int(1,times=nrow(test))
y_hat_naive<-factor(y_hat_naive, levels=c("0","1"))
confusionMatrix(y_hat_naive,test$y)$overall["Accuracy"]

Res_Naive<-data.frame(method="Naive",Accuracy=confusionMatrix(y_hat_naive,test$y)$overall["Accuracy"])

F_Naive<-data.frame(method="Naive",F1=F_meas(y_hat_naive,test$y))

```

The accuracy of this method is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_naive,test$y)$overall["Accuracy"]

```

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results<-inner_join(Res_Naive,F_Naive, by = "method")


```

Finally, the confusion matrix is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_naive,test$y)$table%>%
  kable()

```


## Model 2: Classification tree

In the second model, we are implementing a classification tree and we are experimenting on the complexity parameter (values between 0 and 1 with 0.001 step).

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}
set.seed(1)
grid<-expand.grid(cp=seq(0,1,0.001))     ##tuning for complexity parameter
fittree<-train(y~.,
               data=train,
               method="rpart",
               trControl=control,
               metric=metric,
               tuneGrid=grid)

y_hat_tree<-predict(fittree,test)
confusionMatrix(y_hat_tree,test$y)$overall["Accuracy"]


Res_CART<-data.frame(method="Classification tree",Accuracy=confusionMatrix(y_hat_tree,test$y)$overall["Accuracy"])

F_CART<-data.frame(method="Classification tree",F1=F_meas(y_hat_tree,test$y))
```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_tree,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_tree,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_tree,test$y)))

```

The classification tree appears to produce the same results, as our naive method and is therefore not an improvement.

Finally, the confusion matrix is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_tree,test$y)$table%>%
  kable()

```

The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_CART,F_CART, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```

## Model 3: Neural network

In this model, we are building a neural network.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}
## Neural network

set.seed(1)
fitnn<-train(y~.,
             data=train,
             method="nnet",
             trControl=control,
             metric=metric)
y_hat_nn<-predict(fitnn,test)
confusionMatrix(y_hat_nn,test$y)$overall["Accuracy"]

Res_NN<-data.frame(method="Neural network",Accuracy=confusionMatrix(y_hat_nn,test$y)$overall["Accuracy"])

F_NN<-data.frame(method="Neural network",F1=F_meas(y_hat_nn,test$y))

```



The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_nn,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_nn,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_nn,test$y)))

```


Finally, the confusion matrix is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_nn,test$y)$table%>%
  kable()

```

The Neural network, similar to the CART correctly classifies all individuals with liver disease but also 5 individuals who do not have a liver disease and is therefore an improvement over both Naive method and CART.

The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_NN,F_NN, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```

## Model 4: Adaptive boosting (adaboost)

For our fourth model, we are implementing an adaptive boosting algorithm

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
grid<-expand.grid(nIter=seq(50,250,50),method="adaboost")     ##Tuning for number of trees
fitada<-train(y~.,
              data=train,
              method="adaboost",
              trControl=control,
              tuneGrid=grid,
              metric=metric)

y_hat_ada<-predict(fitada,test)
confusionMatrix(y_hat_ada,test$y)$overall["Accuracy"]

Res_ADA<-data.frame(method="Ada boost",Accuracy=confusionMatrix(y_hat_ada,test$y)$overall["Accuracy"])

F_ADA<-data.frame(method="Ada boost",F1=F_meas(y_hat_ada,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_ada,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_ada,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_ada,test$y)))

```

The adaboost algorithm has an impressive improvement in accuracy, but has also missclassified 4 patients. This can be seen in the confusion matrix:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_ada,test$y)$table%>%
  kable()

```


The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_ADA,F_ADA, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```


## Model 5: Gradient boosting

In this model, we are implementing a gradient boosting algorithm and tuning 3 out of the 4 parameters.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
grid<-expand.grid(n.trees=seq(50,250,50),
                  interaction.depth=1,
                  shrinkage=seq(0.1,0.5,0.1),
                  n.minobsinnode=seq(5,20,1))

gbmFit1 <- train(y ~ ., data = train, 
                 method = "gbm", 
                 trControl = control,
                 verbose = TRUE,
                 tuneGrid = grid,
                 metric=metric)

y_gbm<-predict(gbmFit1,test)
confusionMatrix(y_gbm,test$y)$overall["Accuracy"]

Res_GBM<-data.frame(method="GBM",Accuracy=confusionMatrix(y_gbm,test$y)$overall["Accuracy"])

F_GBM<-data.frame(method="GBM",F1=F_meas(y_gbm,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_gbm,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_gbm,test$y)),paste0("Not defined"), paste0(F_meas(y_gbm,test$y)))

```

Unfortunately, this algorithm is slightly better than the naive method and does not provide any further improvement.

The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_gbm,test$y)$table%>%
  kable()

```


The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_GBM,F_GBM, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```



## Model 6: Support vector machine

For this model, we are implementing an SVM method with a Radial kernel.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
fitsvm<-train(y~.,
              data=train,
              method="svmRadial",
              trControl=control,
              metric=metric)
y_hat_svm<-predict(fitsvm,test)
confusionMatrix(y_hat_svm,test$y)$overall["Accuracy"]

Res_SVM<-data.frame(method="SVM",Accuracy=confusionMatrix(y_hat_svm,test$y)$overall["Accuracy"])

F_SVM<-data.frame(method="SVM",F1=F_meas(y_hat_svm,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_svm,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_svm,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_svm,test$y)))

```

The SVM algorithm produces exactly the same results as our naive method, and is therefore not an improvement.

The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_svm,test$y)$table%>%
  kable()

```


The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_SVM,F_SVM, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```



## Model 7: K nearest neighbors

For the K-nearest neighbors, we will be experimenting with the value k (number of neighbors selected).

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
fitknn<-train(y~.,
              data=train,
              method="knn",
              tuneGrid=expand.grid(k=seq(1:100)),
              trControl=control,
              metric=metric)

y_hat_knn<-predict(fitknn,test)
confusionMatrix(y_hat_knn,test$y)$overall["Accuracy"]

Res_KNN<-data.frame(method="KNN",Accuracy=confusionMatrix(y_hat_knn,test$y)$overall["Accuracy"])

F_KNN<-data.frame(method="KNN",F1=F_meas(y_hat_knn,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_knn,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_knn,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_knn,test$y)))

```


The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_knn,test$y)$table%>%
  kable()

```

The KNN algorithm has the same accuracy as our naive method, but does so by classifying two healthy individuals correctly and missclassifying two sick individuals as healthy. Therefore, it is worse in terms of F1.

The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_KNN,F_KNN, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```


## Model 8: Random Forest

For the random forest algorithm, we are tuning the mtry parameter.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
fitrf<-train(y~.,
             data=train,
             method="rf",
             tuneGrid=expand.grid(mtry=seq(3,20,1)),
             trControl=control,
             metric=metric)

y_hat_rf<-predict(fitrf,test)
confusionMatrix(y_hat_rf,test$y)$overall["Accuracy"]

Res_RF<-data.frame(method="Random forest",Accuracy=confusionMatrix(y_hat_rf,test$y)$overall["Accuracy"])

F_RF<-data.frame(method="Random forest",F1=F_meas(y_hat_rf,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_rf,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_rf,test$y)),paste0("Not defined"), paste0(F_meas(y_hat_rf,test$y)))

```


The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_rf,test$y)$table%>%
  kable()

```

The RF algorithm has a big improvement in terms of accuracy, but 3 patients are also missclassified as healthy.

The results so far:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_RF,F_RF, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```


## Model 9: Extreme Gradiant boosting (Xgboost)

In this section, we are going to implement an extreme gradient boosting algorithm. Due to its complexity and computing time, we are only tuning some of the parameters and keeping the rest fixed.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
                         nrounds = c(50, 75, 100),
                         max_depth = 6:8,
                         min_child_weight = c(2.0, 2.25, 2.5),
                         colsample_bytree = c(0.3, 0.4, 0.5),
                         gamma = 0,
                         subsample = 1)

fitxgb <- train(y ~ ., 
                  data = train,
                  method = "xgbTree",
                  tuneGrid = tune.grid,
                  trControl = control,
                  metric=metric)

y_xgb<-predict(fitxgb,test)
confusionMatrix(y_xgb,test$y)$overall["Accuracy"]

Res_XGB<-data.frame(method="XGBoost",Accuracy=confusionMatrix(y_xgb,test$y)$overall["Accuracy"])

F_XGB<-data.frame(method="XGBoost",F1=F_meas(y_xgb,test$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_xgb,test$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_xgb,test$y)),paste0("Not defined"), paste0(F_meas(y_xgb,test$y)))

```


The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_xgb,test$y)$table%>%
  kable()

```

The algorithm is a slight improvement over our naive method, in terms of accuracy, but not nearly as good as the Random forest algorithm.

The results so far are:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_XGB,F_XGB, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```

## Model 10: Random forest with the original unprocessed dataset

In this final model, we are going to re-train our Random Forest algorithm (best performing one) but this time we are going to use the unprocessed dataset with all the original variables, and excluding the number of extreme values we created.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE, include = FALSE}

set.seed(1)
fitrfS<-train(y~.,
             data=trainS,
             method="rf",
             tuneGrid=expand.grid(mtry=seq(3,20,1)),
             trControl=control,
             metric=metric)

y_hat_rfS<-predict(fitrfS,testS)
confusionMatrix(y_hat_rfS,testS$y)$overall["Accuracy"]

Res_RF_S<-data.frame(method="Random forest Unprocessed",Accuracy=confusionMatrix(y_hat_rfS,testS$y)$overall["Accuracy"])

F_RF_S<-data.frame(method="Random forest",F1=F_meas(y_hat_rfS,testS$y))

```

The accuracy of this model is:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_rfS,testS$y)$overall["Accuracy"]

```

The F1 score is: 

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

ifelse(is.na(F_meas(y_hat_rfS,testS$y)),paste0("Not defined"), paste0(F_meas(y_hat_rfS,testS$y)))

```


The confusion matrix is shown in the following table:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

confusionMatrix(y_hat_rfS,testS$y)$table%>%
  kable()

```

The RF algorithm with the unprocessed dataset is not nearly as good as our implementation, proving that the steps we took in feature engineering were beneficial in the model development phase.

The final results are:

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

Results1<-inner_join(Res_RF_S,F_RF_S, by = "method")
Results<-rbind(Results,Results1)
rm(Results1)

Results %>%
  kable()

```

## Variable importance in the Random forest model (feature engineered dataset)

Before concluding this session, we are going to plot the variable importance of our final algorithm.

```{r, echo=FALSE, eval = TRUE, warning=FALSE, message=FALSE}

plot(varImp(fitrf),
     main="Variable importance plot Random Forest",
     xlab="Variables",
     ylab="Variable importance")

```

\newpage
# Conclusion


In this chapter, we will be discussing our methods, models and results, explain some of the limitations of our research and conclude this project with some directions for future research.

## Discussion of results

In chapter 5, we have developed ten machine learning models (9 original models + 1 model for testing feature engineering steps). From those models, the best performing one was the Random forest, with the second best model being 2.5% less accurate (adaboost). In terms of missclassifying sick individuals as healthy, our model managed to missclassify only 3 out of the 83 people with a liver disease, which is quite accurate. Compared to our naive approach (classify all people as sick), we see a trade-off: the increase in accuracy vs. the incorectly classified people. Given the fact that it is a medical application and false negatives are more important than false positives, our naive approach is only accurate 71% of the times (mainly due to the imbalanced dataset) and therefore we suggest that the RF classifier is an improvement over the naive method.

From our analysis, we can also see that the variable, which had the biggest impact in the model improvement, was the age. This finding is counter-intuitive as our EDA did not show a strong relationship between the age and the existence of a liver disease. Additionally, the variable we have created has low importance (8%) and the gender appears to play no role in detecting the liver disease (in line with literature). Interestingly, using all the variables as features proved to be less valueable than using only some of them (conveying different type of information).

Overall, the best performing model had a substantial improvement (13%) over our first benchmark model (Naive method) and predicts with high accuracy the existence of a liver disease in the unseen test set (Accuracy = 81.03%).

## Limitations

The algorithm and research presented, yielded some interesting results but are not without limitations. To begin with, for some of the algorithms, due to the computation time, only a small subset of parameters was studied; however, we could potentially use virtual machines for estimating a larger range of parameters, which could lead to further improvements. Furthermore, as part of the research, we only implemented a small number of algorithms. In that regard, an alternative algorithm might prove to be more robust in predicting the existence of liver disease in people. Finally, given that the dataset is based on the records of Indian patients, the generelizability of the methodology presented needs to be tested with other (international) patients.

##Further research

In this project, several methods for creating a liver disease classifier have been studied, with decent results. In future research, new methods for estimating the existence of liver disease should be examined and benchmarked against the original models. Additionally, we propose a more in-depth investigation of the age effect, to identify some evidence on the importance of this feature. Finally, despite the low importance of the number of extreme blood test results that was developed, some tuning on what should be considered "normal range of values" should also be examined.



